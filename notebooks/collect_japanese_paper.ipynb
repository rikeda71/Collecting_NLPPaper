{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP20●●からの論文情報抽出\n",
    "## 抽出対象\n",
    "- タイトル\n",
    "- 著者\n",
    "- 論文のURL\n",
    "- PDF中のintroduction\n",
    "- カテゴリ\n",
    "    - タスク\n",
    "    - ポスター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語処理学会の発表論文集のページの分析\n",
    "- URL\n",
    "    - https://www.anlp.jp/proceedings/annual_meeting/20●●/ のようになっている\n",
    "    - ex.) 2019なら https://www.anlp.jp/proceedings/annual_meeting/2019/\n",
    "\n",
    "- ページのデザイン\n",
    "    - 2014 ~ 2019が現行のデザイン\n",
    "    - ~2013は旧デザイン\n",
    "    - oral発表のセッション名はclass=\"program_oral\"で設定されているみたい.\n",
    "        - ただし，年度によって，セッション名が違う．\n",
    "        - タスクが多様化したり，増えたりすることが理由か．これの対処は必要そう\n",
    "        - セッション名をそのまんま分類先のクラスとして考える必要はなく，ある程度まとめてもいいと思う．\n",
    "            - 例えば，知識獲得，情報抽出，固有表現抽出はIE（Information Extraction）クラスにするみたいな"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2014〜は以下の方法で取得できるが，〜2013は取得できない．抽出したいなら，エラーの検証が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.anlp.jp/proceedings/annual_meeting/2019/')\n",
    "# r = requests.get('https://www.anlp.jp/proceedings/annual_meeting/2013/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding = r.apparent_encoding\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発表情報の抽出方法の検討\n",
    "- class=session1とclass=session2で発表は抽出できそう\n",
    "    - session1と2の違いは，背景色\n",
    "    - session1：白，session2：グレー\n",
    "- フィルタリング対象\n",
    "    - チュートリアル\n",
    "        - チュートリアルはidがTから始まるため，これを見ればフィルタリング可能\n",
    "    - 招待講演\n",
    "        - 招待講演はidがinvited+数字 か I+数字で決まるため(2019のみ)，これを利用する\n",
    "    - テーマセッション\n",
    "        - テーマセッションは文字列中に”テーマセッション”と書かれるため，これを利用する\n",
    "    - ワークショップ\n",
    "        - idにworkshopという文字列が含まれている\n",
    "    - その他\n",
    "        - sympo（シンポジウムかな)\n",
    "        - sponser_evening\n",
    "- まとめて，class=session_titleの中で，id=英大文字 + 数字出ないものをフィルタリングし，タイトルの文字列が英大文字で始まらないもの（チュートリアルと招待講演）を除けば，論文集は抽出できそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = soup.find_all(class_='session1')\n",
    "session2 = soup.find_all(class_='session2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"session_title\" id=\"T1\">チュートリアル(1)</span>\n",
      "<span class=\"session_title\" id=\"E6\">E6:知識獲得・情報抽出(3)</span>\n"
     ]
    }
   ],
   "source": [
    "print(session1[0].find(class_='session_title'))\n",
    "print(session1[-1].find(class_='session_title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"session_title\" id=\"A1\">A1:機械翻訳(1)</span>\n",
      "<span class=\"session_title\" id=\"C1\">C1:実験データに基づく言語学(1)</span>\n",
      "<span class=\"session_title\" id=\"E1\">E1:知識獲得・情報抽出(1)</span>\n",
      "<span class=\"session_title\" id=\"A2\">A2:機械翻訳(2)</span>\n",
      "<span class=\"session_title\" id=\"C2\">C2:言語資源(1)</span>\n",
      "<span class=\"session_title\" id=\"E2\">E2:音声言語処理</span>\n",
      "<span class=\"session_title\" id=\"A3\">A3:埋め込み表現(1)</span>\n",
      "<span class=\"session_title\" id=\"C3\">C3:実験データに基づく言語学(2)</span>\n",
      "<span class=\"session_title\" id=\"E3\">E3:知識獲得・情報抽出(2)</span>\n",
      "<span class=\"session_title\" id=\"P1\">P1:ポスター(1)</span>\n",
      "<span class=\"session_title\" id=\"P3\">P3:ポスター(2)</span>\n",
      "<span class=\"session_title\" id=\"A4\">A4:テーマセッション: 実世界にグラウンドされた言語処理</span>\n",
      "<span class=\"session_title\" id=\"C4\">C4:テーマセッション: 言語教育と言語処理の接点</span>\n",
      "<span class=\"session_title\" id=\"E4\">E4:テーマセッション: 試験問題をベンチマークとする言語処理</span>\n",
      "<span class=\"session_title\" id=\"A5\">A5:機械翻訳(3)</span>\n",
      "<span class=\"session_title\" id=\"C5\">C5:言語資源(2)</span>\n",
      "<span class=\"session_title\" id=\"E5\">E5:機械学習</span>\n",
      "<span class=\"session_title\" id=\"P5\">P5:ポスター(3)</span>\n",
      "<span class=\"session_title\" id=\"P7\">P7:ポスター(4)</span>\n",
      "<span class=\"session_title\" id=\"A6\">A6:機械翻訳(4)</span>\n",
      "<span class=\"session_title\" id=\"C6\">C6:語彙資源・辞書</span>\n",
      "<span class=\"session_title\" id=\"E6\">E6:知識獲得・情報抽出(3)</span>\n"
     ]
    }
   ],
   "source": [
    "for s1 in session1:\n",
    "    session_name = s1.find(class_='session_title')\n",
    "    if re.search(r'[A-Z]\\d+', session_name['id']) is None:\n",
    "        continue\n",
    "    if re.search(r'[A-Z]\\d+', session_name.text) is None:\n",
    "        continue\n",
    "    print(session_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ただし，セッションを抽出しても，セッションに紐づく論文はうまく抽出できない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"session1\">\n",
       "<div class=\"session_header\"><span class=\"session_title\" id=\"E6\">E6:知識獲得・情報抽出(3)</span>\n",
       "　　3月15日(金) 15:00-16:40   ES024　　座長: 山田一郎(NHK)<br/></div>\n",
       "<table>\n",
       "<tr><td class=\"pid\"><span id=\"E6-1\">E6-1</span></td>\n",
       "<td><span class=\"title\">Conditional VAEに基づく多様性を考慮したイベント予測</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-1.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○清丸寛一, 大村和正, 村脇有吾, 河原大輔, 黒橋禎夫 (京大)</td></tr>\n",
       "<tr><td class=\"pid\"><span id=\"E6-2\">E6-2</span></td>\n",
       "<td><span class=\"title\">ニューラルネットワークを用いたトピック遷移モデリングに関する検討</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-2.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○内田脩斗, 吉川大弘, 古橋武 (名大)</td></tr>\n",
       "<tr><td class=\"pid\"><span id=\"E6-3\">E6-3</span></td>\n",
       "<td><span class=\"title\">単語の分散表現を用いた日本語イベント連鎖の自動構築</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-3.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○瀧下祥, Rafal Rzepka, 荒木健治 (北大)</td></tr>\n",
       "<tr><td class=\"pid\"><span id=\"E6-4\">E6-4</span></td>\n",
       "<td><span class=\"title\">TextRankと依存関係情報の組み合わせによるArgument Componentの分類</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-4.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○出口衛, 山口和紀 (東大)</td></tr>\n",
       "<tr><td class=\"pid\"><span id=\"E6-5\">E6-5</span></td>\n",
       "<td><span class=\"title\">共有タスクにおけるGA重み付け加重投票を用いた属性値アンサンブル</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-5.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○中山功太 (理研AIP/豊橋技科大), 小林暁雄, 関根聡 (理研AIP)</td></tr>\n",
       "</table>\n",
       "<div class=\"session_footer\"><a href=\"#session_table\">(セッション一覧へ戻る)</a></div>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.find('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### セッションに基づく論文の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in soup.find_all('tr'):\n",
    "    if tr.find_all('td') != []:\n",
    "        # セッション番号: session 論文タイトル: title\n",
    "        pid = tr.find('td', class_='pid')\n",
    "        if pid is not None and pid.find('span') is not None:\n",
    "            paper_info = tr.find_all('td')\n",
    "            session = paper_info[0].text[:2]\n",
    "            title = paper_info[1].text\n",
    "            print(session, title)\n",
    "        # 論文URL: url\n",
    "        elif tr.find('a') is not None and tr.find('a')['href'][0] != '#':\n",
    "            url = tr.find('a')['href']\n",
    "            authors_str = re.sub(r'\\s\\(.+\\)', '', tr.find_all('td')[1].text)\n",
    "            authors = authors_str.replace('○', '').split(', ')\n",
    "            print(url)\n",
    "            print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFからのintroductionの抽出\n",
    "- pdfからの情報抽出には，pdfminerを用いる\n",
    "- pdfからの情報抽出は以下のように，くっついて出力されるので，ルールにより，ある程度のところで切り取る\n",
    "    - ルール\n",
    "        - 1 + ['はじめに', '序論', '背景', '背景と目的', 'Introduction']を探す\n",
    "            - ある場合はそれを始点として情報抽出\n",
    "            - ない場合は，1ページ目全てを保存する\n",
    "        - \".2\" or \"。2\"を探す\n",
    "            - ある場合は，それを終点として情報抽出\n",
    "            - ない場合は，1ページ目全てを保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright(C) 2019 The Association for Natural Language Processing.\n",
      "All Rights Reserved.\n",
      "\n",
      "深層言語表現モデルに対するマルチヘッド・多層アテンションによる英語文法誤り検出金子正弘　小町守首都大学東京kaneko-masahiro@ed.tmu.ac.jpkomachi@tmu.ac.jp1はじめにニューラルネットワークは大規模コーパスで学習された際に，最も真価を発揮することが知られている．一般的に生コーパスは大規模に存在しており，生コーパスだけから学習することが可能である言語表現モデル1はNLPにおいて最も大規模コーパスの恩恵を受けることが可能なモデルの一つと言える。そのため，言語表現モデルを様々なタスクに活用する研究が盛んに行われている．文法誤り検出における大規模生コーパスを活用した研究としては，生コーパスから擬似文法誤りデータを作成し学習データとして活用する研究[5,2]などがある．しかし，我々の知る限り大規模生コーパスで事前学習された言語表現モデルを文法誤り検出に活用した研究は存在しない．事前学習されたモデルをファインチューニングする研究の多くは，事前学習されたモデルの最終層の情報だけを用いて予測を行っている．一方で，ニューラルネットワークは層ごとに異なる表現を学習していることが知られている．例えば，浅い層が局所的な統語関係の学習に最適であり，深い層が長距離関係をモデル化すること[4]や，文脈付き単語分散表現の学習の際に，タスクごとに異なる層の情報を活用することが精度向上に繋がること[3]が報告されている．文法誤り検出などの統語的な情報が重要となるタスクでは，表層情報は深層情報と同様に重要であると考えられる．そのため，様々な層の情報を活用可能なモデルが求められる．これらのことから，我々は事前学習された深層言語表現モデルの最終層だけを用いるのではなく文法誤り検出に適した表現を多様な層から構築可能なモデルを提案する．図1は提案手法であるマルチヘッド・多層アテンションを示している．マルチヘッドは入力ベク1言語モデル，マスク言語モデル，次文を予測することで言語表現を学習するモデルなど[1]図1:マルチヘッド・多層アテンションモデル．トルの異なる部分空間の情報を別々に扱い結合するため，単一ヘッドと比較して様々な層の情報を活用するように学習すると考えられる．そのため，本モデルでは各層に対してマルチヘッドアテンションを計算する．本稿の主要な貢献は以下の3つである：(cid:15)3つの文法誤り検出データセット(FCE，CoNLL14とJFLEG)において，提案手法が既存手法を大幅に上回り世界最高精度を達成した．(cid:15)大規模コーパスで事前学習された言語表現モデルに対して，与えられたタスクに最適な表現を各層から構築するマルチヘッド・多層アテンションを提案した．(cid:15)分析から，層に対するアテンションのヘッド数を増やすことで多様な層から情報を獲得することを示した．さらに，様々な層に対してアテンションを計算することが入力文中のさまざまなトークンの情報を活用する効果があることも明らかにした．2言語表現モデル事前学習されたBidirectionalEncoderRepresenta-tionsfromTransformers(BERT)[1]に対して，マル― 446 ―言語処理学会 第25回年次大会 発表論文集 (2019年3月)\f",
      "\n",
      "-----------------------------\n",
      "ニューラルネットワークは大規模コーパスで学習された際に，最も真価を発揮することが知られている．一般的に生コーパスは大規模に存在しており，生コーパスだけから学習することが可能である言語表現モデル1はNLPにおいて最も大規模コーパスの恩恵を受けることが可能なモデルの一つと言える。そのため，言語表現モデルを様々なタスクに活用する研究が盛んに行われている．文法誤り検出における大規模生コーパスを活用した研究としては，生コーパスから擬似文法誤りデータを作成し学習データとして活用する研究[5,2]などがある．しかし，我々の知る限り大規模生コーパスで事前学習された言語表現モデルを文法誤り検出に活用した研究は存在しない．事前学習されたモデルをファインチューニングする研究の多くは，事前学習されたモデルの最終層の情報だけを用いて予測を行っている．一方で，ニューラルネットワークは層ごとに異なる表現を学習していることが知られている．例えば，浅い層が局所的な統語関係の学習に最適であり，深い層が長距離関係をモデル化すること[4]や，文脈付き単語分散表現の学習の際に，タスクごとに異なる層の情報を活用することが精度向上に繋がること[3]が報告されている．文法誤り検出などの統語的な情報が重要となるタスクでは，表層情報は深層情報と同様に重要であると考えられる．そのため，様々な層の情報を活用可能なモデルが求められる．これらのことから，我々は事前学習された深層言語表現モデルの最終層だけを用いるのではなく文法誤り検出に適した表現を多様な層から構築可能なモデルを提案する．図1は提案手法であるマルチヘッド・多層アテンションを示している．マルチヘッドは入力ベク1言語モデル，マスク言語モデル，次文を予測することで言語表現を学習するモデルなど[1]図1:マルチヘッド・多層アテンションモデル．トルの異なる部分空間の情報を別々に扱い結合するため，単一ヘッドと比較して様々な層の情報を活用するように学習すると考えられる．そのため，本モデルでは各層に対してマルチヘッドアテンションを計算する．本稿の主要な貢献は以下の3つである：(cid:15)3つの文法誤り検出データセット(FCE，CoNLL14とJFLEG)において，提案手法が既存手法を大幅に上回り世界最高精度を達成した．(cid:15)大規模コーパスで事前学習された言語表現モデルに対して，与えられたタスクに最適な表現を各層から構築するマルチヘッド・多層アテンションを提案した．(cid:15)分析から，層に対するアテンションのヘッド数を増やすことで多様な層から情報を獲得することを示した．さらに，様々な層に対してアテンションを計算することが入力文中のさまざまなトークンの情報を活用する効果があることも明らかにした\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "\n",
    "r = requests.get(\n",
    "    # 'https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/A1-1.pdf'\n",
    "    #'https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/E3-3.pdf'\n",
    "    #'https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/P1-1.pdf'\n",
    "    'https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/P1-36.pdf'\n",
    ")\n",
    "# BytesIOとStringIOはbyteやstringをIOオブジェクト，つまりファイルオープンしたオブジェクトと等価にする\n",
    "instr = BytesIO()\n",
    "instr.write(r.content)\n",
    "outstr = StringIO()\n",
    "manager = PDFResourceManager()\n",
    "laparams = LAParams()\n",
    "laparams.detect_vertical = True\n",
    "with TextConverter(manager, outstr, codec='utf-8', laparams=laparams) as device:\n",
    "    interpreter = PDFPageInterpreter(manager, device)\n",
    "    for page in PDFPage.get_pages(instr, set(), maxpages=1, caching=True, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "        onepage = outstr.getvalue()\n",
    "        intro = ''\n",
    "        for chap in ['はじめに', '序論', '背景', '背景と目的', 'Introduction']:\n",
    "            chap_name = '1{}'.format(chap)\n",
    "            if chap_name in onepage:\n",
    "                if '．2' in onepage:\n",
    "                    intro = onepage[onepage.find(chap_name) + len(chap_name): onepage.find('．2')]\n",
    "                elif '.2' in onepage:\n",
    "                    intro = onepage[onepage.find(chap_name) + len(chap_name): onepage.find('.2')]\n",
    "                elif '。2' in onepage:\n",
    "                    intro = onepage[onepage.find(chap_name) + len(chap_name): onepage.find('。2')]\n",
    "                break\n",
    "        intro = onepage if intro == '' else intro\n",
    "        print(onepage)\n",
    "        print('-----------------------------')\n",
    "        print(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['はじめに', '序論', '背景', '背景と目的', 'Introduction']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
