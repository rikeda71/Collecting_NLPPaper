{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP20●●からの論文情報抽出\n",
    "## 抽出対象\n",
    "- タイトル\n",
    "- 著者\n",
    "- 論文のURL\n",
    "- PDF中のintroduction\n",
    "- カテゴリ\n",
    "    - タスク\n",
    "    - ポスター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語処理学会の発表論文集のページの分析\n",
    "- URL\n",
    "    - https://www.anlp.jp/proceedings/annual_meeting/20●●/ のようになっている\n",
    "    - ex.) 2019なら https://www.anlp.jp/proceedings/annual_meeting/2019/\n",
    "\n",
    "- ページのデザイン\n",
    "    - 2014 ~ 2019が現行のデザイン\n",
    "    - ~2013は旧デザイン\n",
    "    - oral発表のセッション名はclass=\"program_oral\"で設定されているみたい.\n",
    "        - ただし，年度によって，セッション名が違う．\n",
    "        - タスクが多様化したり，増えたりすることが理由か．これの対処は必要そう\n",
    "        - セッション名をそのまんま分類先のクラスとして考える必要はなく，ある程度まとめてもいいと思う．\n",
    "            - 例えば，知識獲得，情報抽出，固有表現抽出はIE（Information Extraction）クラスにするみたいな"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2014〜は以下の方法で取得できるが，〜2013は取得できない．抽出したいなら，エラーの検証が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.anlp.jp/proceedings/annual_meeting/2019/')\n",
    "# r = requests.get('https://www.anlp.jp/proceedings/annual_meeting/2013/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding = r.apparent_encoding\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発表情報の抽出方法の検討\n",
    "- class=session1とclass=session2で発表は抽出できそう\n",
    "    - session1と2の違いは，背景色\n",
    "    - session1：白，session2：グレー\n",
    "- フィルタリング対象\n",
    "    - チュートリアル\n",
    "        - チュートリアルはidがTから始まるため，これを見ればフィルタリング可能\n",
    "    - 招待講演\n",
    "        - 招待講演はidがinvited+数字 か I+数字で決まるため(2019のみ)，これを利用する\n",
    "    - テーマセッション\n",
    "        - テーマセッションは文字列中に”テーマセッション”と書かれるため，これを利用する\n",
    "    - ワークショップ\n",
    "        - idにworkshopという文字列が含まれている\n",
    "    - その他\n",
    "        - sympo（シンポジウムかな)\n",
    "        - sponser_evening\n",
    "- まとめて，class=session_titleの中で，id=英大文字 + 数字出ないものをフィルタリングし，タイトルの文字列が英大文字で始まらないもの（チュートリアルと招待講演）を除けば，論文集は抽出できそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = soup.find_all(class_='session1')\n",
    "session2 = soup.find_all(class_='session2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"session_title\" id=\"T1\">チュートリアル(1)</span>\n",
      "<span class=\"session_title\" id=\"E6\">E6:知識獲得・情報抽出(3)</span>\n"
     ]
    }
   ],
   "source": [
    "print(session1[0].find(class_='session_title'))\n",
    "print(session1[-1].find(class_='session_title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"session_title\" id=\"A1\">A1:機械翻訳(1)</span>\n",
      "<span class=\"session_title\" id=\"C1\">C1:実験データに基づく言語学(1)</span>\n",
      "<span class=\"session_title\" id=\"E1\">E1:知識獲得・情報抽出(1)</span>\n",
      "<span class=\"session_title\" id=\"A2\">A2:機械翻訳(2)</span>\n",
      "<span class=\"session_title\" id=\"C2\">C2:言語資源(1)</span>\n",
      "<span class=\"session_title\" id=\"E2\">E2:音声言語処理</span>\n",
      "<span class=\"session_title\" id=\"A3\">A3:埋め込み表現(1)</span>\n",
      "<span class=\"session_title\" id=\"C3\">C3:実験データに基づく言語学(2)</span>\n",
      "<span class=\"session_title\" id=\"E3\">E3:知識獲得・情報抽出(2)</span>\n",
      "<span class=\"session_title\" id=\"P1\">P1:ポスター(1)</span>\n",
      "<span class=\"session_title\" id=\"P3\">P3:ポスター(2)</span>\n",
      "<span class=\"session_title\" id=\"A4\">A4:テーマセッション: 実世界にグラウンドされた言語処理</span>\n",
      "<span class=\"session_title\" id=\"C4\">C4:テーマセッション: 言語教育と言語処理の接点</span>\n",
      "<span class=\"session_title\" id=\"E4\">E4:テーマセッション: 試験問題をベンチマークとする言語処理</span>\n",
      "<span class=\"session_title\" id=\"A5\">A5:機械翻訳(3)</span>\n",
      "<span class=\"session_title\" id=\"C5\">C5:言語資源(2)</span>\n",
      "<span class=\"session_title\" id=\"E5\">E5:機械学習</span>\n",
      "<span class=\"session_title\" id=\"P5\">P5:ポスター(3)</span>\n",
      "<span class=\"session_title\" id=\"P7\">P7:ポスター(4)</span>\n",
      "<span class=\"session_title\" id=\"A6\">A6:機械翻訳(4)</span>\n",
      "<span class=\"session_title\" id=\"C6\">C6:語彙資源・辞書</span>\n",
      "<span class=\"session_title\" id=\"E6\">E6:知識獲得・情報抽出(3)</span>\n"
     ]
    }
   ],
   "source": [
    "for s1 in session1:\n",
    "    session_name = s1.find(class_='session_title')\n",
    "    if re.search(r'[A-Z]\\d+', session_name['id']) is None:\n",
    "        continue\n",
    "    if re.search(r'[A-Z]\\d+', session_name.text) is None:\n",
    "        continue\n",
    "    print(session_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ただし，セッションを抽出しても，セッションに紐づく論文はうまく抽出できない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"session1\">\n",
       "<div class=\"session_header\"><span class=\"session_title\" id=\"E6\">E6:知識獲得・情報抽出(3)</span>\n",
       "　　3月15日(金) 15:00-16:40   ES024　　座長: 山田一郎(NHK)<br/></div>\n",
       "<table>\n",
       "<tr><td class=\"pid\"><span id=\"E6-1\">E6-1</span></td>\n",
       "<td><span class=\"title\">Conditional VAEに基づく多様性を考慮したイベント予測</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-1.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○清丸寛一, 大村和正, 村脇有吾, 河原大輔, 黒橋禎夫 (京大)</td></tr>\n",
       "<tr><td class=\"pid\"><span id=\"E6-2\">E6-2</span></td>\n",
       "<td><span class=\"title\">ニューラルネットワークを用いたトピック遷移モデリングに関する検討</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-2.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○内田脩斗, 吉川大弘, 古橋武 (名大)</td></tr>\n",
       "<tr><td class=\"pid\"><span id=\"E6-3\">E6-3</span></td>\n",
       "<td><span class=\"title\">単語の分散表現を用いた日本語イベント連鎖の自動構築</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-3.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○瀧下祥, Rafal Rzepka, 荒木健治 (北大)</td></tr>\n",
       "<tr><td class=\"pid\"><span id=\"E6-4\">E6-4</span></td>\n",
       "<td><span class=\"title\">TextRankと依存関係情報の組み合わせによるArgument Componentの分類</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-4.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○出口衛, 山口和紀 (東大)</td></tr>\n",
       "<tr><td class=\"pid\"><span id=\"E6-5\">E6-5</span></td>\n",
       "<td><span class=\"title\">共有タスクにおけるGA重み付け加重投票を用いた属性値アンサンブル</span></td></tr>\n",
       "<tr><td><a href=\"pdf_dir/E6-5.pdf\"><img src=\"html/img/pdf.png\"/></a></td>\n",
       "<td>○中山功太 (理研AIP/豊橋技科大), 小林暁雄, 関根聡 (理研AIP)</td></tr>\n",
       "</table>\n",
       "<div class=\"session_footer\"><a href=\"#session_table\">(セッション一覧へ戻る)</a></div>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### セッションに基づく論文の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in soup.find_all('tr'):\n",
    "    if tr.find_all('td') != []:\n",
    "        # セッション番号: session 論文タイトル: title\n",
    "        pid = tr.find('td', class_='pid')\n",
    "        if pid is not None and pid.find('span') is not None:\n",
    "            paper_info = tr.find_all('td')\n",
    "            session = paper_info[0].text[:2]\n",
    "            title = paper_info[1].text\n",
    "            print(session, title)\n",
    "        # 論文URL: url\n",
    "        elif tr.find('a') is not None and tr.find('a')['href'][0] != '#':\n",
    "            print(tr.find('a'))\n",
    "            url = tr.find('a')['href']\n",
    "            authors_str = re.sub(r'\\s\\(.+\\)', '', tr.find_all('td')[1].text)\n",
    "            authors = authors_str.replace('○', '').split(', ')\n",
    "            print(url)\n",
    "            print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFからのintroductionの抽出\n",
    "- pdfからの情報抽出には，pdfminerを用いる\n",
    "- pdfからの情報抽出は以下のように，くっついて出力されるので，ルールにより，ある程度のところで切り取る\n",
    "    - ルール\n",
    "        - 1 + ['はじめに', '序論', '背景', '背景と目的', 'Introduction']を探す\n",
    "            - ある場合はそれを始点として情報抽出\n",
    "            - ない場合は，1ページ目全てを保存する\n",
    "        - \".2\" or \"。2\"を探す\n",
    "            - ある場合は，それを終点として情報抽出\n",
    "            - ない場合は，1ページ目全てを保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "\n",
    "r = requests.get(\n",
    "    # 'https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/A1-1.pdf'\n",
    "    #'https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/E3-3.pdf'\n",
    "    #'https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/P1-1.pdf'\n",
    "    'https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/P1-36.pdf'\n",
    ")\n",
    "# BytesIOとStringIOはbyteやstringをIOオブジェクト，つまりファイルオープンしたオブジェクトと等価にする\n",
    "instr = BytesIO()\n",
    "instr.write(r.content)\n",
    "outstr = StringIO()\n",
    "manager = PDFResourceManager()\n",
    "laparams = LAParams()\n",
    "laparams.detect_vertical = True\n",
    "with TextConverter(manager, outstr, codec='utf-8', laparams=laparams) as device:\n",
    "    interpreter = PDFPageInterpreter(manager, device)\n",
    "    for page in PDFPage.get_pages(instr, set(), maxpages=1, caching=True, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "        onepage = outstr.getvalue()\n",
    "        intro = ''\n",
    "        for chap in ['はじめに', '序論', '背景', '背景と目的', 'Introduction']:\n",
    "            chap_name = '1{}'.format(chap)\n",
    "            if chap_name in onepage:\n",
    "                if '．2' in onepage:\n",
    "                    intro = onepage[onepage.find(chap_name) + len(chap_name): onepage.find('．2')]\n",
    "                elif '.2' in onepage:\n",
    "                    intro = onepage[onepage.find(chap_name) + len(chap_name): onepage.find('.2')]\n",
    "                elif '。2' in onepage:\n",
    "                    intro = onepage[onepage.find(chap_name) + len(chap_name): onepage.find('。2')]\n",
    "                break\n",
    "        intro = onepage if intro == '' else intro\n",
    "        print(onepage)\n",
    "        print('-----------------------------')\n",
    "        print(intro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
